{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce9fa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheila/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sheila/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sheila/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sheila/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sheila/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sheila/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade tf_slim\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tf_slim as slim\n",
    "import numpy.random as rnd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque # container em forma de lista, com rapida insercao e remocao nas duas extremidades \n",
    "from ns3gym import ns3env\n",
    "\n",
    "#from sklearn.preprocessing import label_binarize\n",
    "#from keras.layers import Dense, Flatten, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d054e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_random_seed(seed)#tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a7cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got new port for ns3gm interface:  5205\n",
      "Observation space:  Box(0, 100, (5,), uint64) uint64\n",
      "Action space:  Box(0, 100, (5,), uint64) uint64\n",
      "\n",
      "\n",
      "Numero de Estados:  5\n",
      "Numero de Acoes:  5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ns3-v0') # environment ID \n",
    "\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "print(\"Observation space: \", ob_space,  ob_space.dtype)\n",
    "print(\"Action space: \", ac_space, ac_space.dtype)\n",
    "\n",
    "# numero de estados\n",
    "state_size = ob_space.shape[0]\n",
    "\n",
    "# numero de acoes\n",
    "action_size = ac_space.shape[0]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Numero de Estados: \", state_size)\n",
    "print(\"Numero de Acoes: \", action_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe323db",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfEpisodes = 50 #300 #200\n",
    "numTimeSlots = 50 #600 #495\n",
    "\n",
    "discount_rate = 0.618 # gamma 0.618; 0.528\n",
    "learning_rate = 0.7 # Learning rate 0.7\n",
    "\n",
    "# Epsilon greedy parameters\n",
    "epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "max_epsilon = 1 # You can't explore more than 100% of the time\n",
    "min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n",
    "decay = 0.01\n",
    "epsilon_vec = np.zeros((numOfEpisodes,))\n",
    "\n",
    "maxNofCollisions = 3\n",
    "\n",
    "batch_size = 128 # tamanho do batch 128\n",
    "\n",
    "MIN_REPLAY_SIZE = 1000 # tamanho minimo da memoria de repeticao\n",
    "\n",
    "inputQueues = 1\n",
    "cwSize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0108c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(inputQueues, cwSize, loss='mse'):\n",
    "    learning_rate = 0.01\n",
    "    init = tf.keras.initializers.he_uniform()#tf.keras.initializers.HeUniform()\n",
    "    model = keras.Sequential()\n",
    "    # Input layer - número dos estados de entrada \n",
    "    model.add(keras.layers.Dense(inputQueues, input_shape=(inputQueues,), activation='relu', kernel_initializer=init))\n",
    "    # Output layer - número de ações no ambiente\n",
    "    model.add(keras.layers.Dense(cwSize, activation='linear', kernel_initializer=init))\n",
    "    #model.add(keras.layers.Flatten())\n",
    "    \n",
    "    if(loss=='huber'):\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=tf.keras.losses.Huber(), metrics=['accuracy'])\n",
    "        print('Using Huber loss...', \"\\n\")\n",
    "    elif(loss=='mse'):\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['accuracy'])\n",
    "        print('Using MSE loss...', \"\\n\")\n",
    "    else:\n",
    "        print('Loss not defined...', \"\\n\")\n",
    "    return model\n",
    "\n",
    "def train(replay_memory, model, model_target, done, batch_size, acc_history=[], loss_history=[]):\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    c_states = np.array([transition[0] for transition in mini_batch])    \n",
    "    current_qs_list  = model.predict(c_states)\n",
    "    #current_qs_list0 = model.predict(c_states[:,0]-c_states[:,1])\n",
    "    #current_qs_list1 = model.predict(c_states[:,1]-c_states[:,2])\n",
    "    #current_qs_list2 = model.predict(c_states[:,2]-c_states[:,3])\n",
    "    #current_qs_list3 = model.predict(c_states[:,3]-c_states[:,4])\n",
    "    n_states = np.array([transition[3] for transition in mini_batch]) \n",
    "    \n",
    "    future_qs_list = model_target.predict(n_states)\n",
    "    #future_qs_list0 = model_target.predict(n_states[:,0]-n_states[:,1])\n",
    "    #future_qs_list1 = model_target.predict(n_states[:,1]-n_states[:,2])\n",
    "    #future_qs_list2 = model_target.predict(n_states[:,2]-n_states[:,3])\n",
    "    #future_qs_list3 = model_target.predict(n_states[:,3]-n_states[:,4])\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (c_state, actionVec, reward, n_state, done) in enumerate(mini_batch):\n",
    "        \n",
    "        if not done:\n",
    "            max_future_q = reward + discount_rate * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        \n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[actionVec] = max_future_q\n",
    "\n",
    "        X.append(c_state)\n",
    "        Y.append(current_qs)\n",
    "\n",
    "    # Train model with a mini-batch.\n",
    "    history = model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)\n",
    "\n",
    "    # Store metrics.\n",
    "    acc_history.append(history.history['acc'][0])\n",
    "    loss_history.append(history.history['loss'][0])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3c593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Network\n",
      "Using MSE loss... \n",
      "\n",
      "Target Network\n",
      "Using MSE loss... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction Network\")\n",
    "# The first model makes the predictions for Q-values which are used to make a action.\n",
    "model = createModel(inputQueues, cwSize)\n",
    "\n",
    "print(\"Target Network\")\n",
    "# Build a target model for the prediction of future rewards.\n",
    "model_target = createModel(inputQueues, cwSize)\n",
    "# Initialize target model's weights.\n",
    "model_target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2999d4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce5a0d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "linear-wireless-mesh|123\n",
      "Episode: 1\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  7254\n",
      "Episode: 2\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  7007\n",
      "Episode: 3\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  6829\n",
      "Episode: 4\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  5840\n",
      "Episode: 5\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  9468\n",
      "Episode: 6\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  5713\n",
      "Episode: 7\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  9838\n",
      "Episode: 8\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  8457\n",
      "Episode: 9\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  5261\n",
      "Episode: 10\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  5245\n",
      "Episode: 11\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  5768\n",
      "Episode: 12\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  6792\n",
      "Episode: 13\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  6906\n",
      "Episode: 14\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  9140\n",
      "Episode: 15\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  9932\n",
      "Episode: 16\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  5218\n",
      "Episode: 17\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  9598\n",
      "Episode: 18\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  6629\n",
      "Episode: 19\n",
      "linear-wireless-mesh|123\n",
      "Got new port for ns3gm interface:  9465\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (1,) but got array with shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-38c237785adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mMIN_REPLAY_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Is it the end of the episode?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-40304d69135b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(replay_memory, model, model_target, done, batch_size, acc_history, loss_history)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mc_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcurrent_qs_list\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#current_qs_list0 = model.predict(c_states[:,0]-c_states[:,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#current_qs_list1 = model.predict(c_states[:,1]-c_states[:,2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;31m# generate symbolic tensors).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1060\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/environment_py3_7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (1,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "acc_history = []\n",
    "loss_history = []\n",
    "\n",
    "episode_reward = np.zeros((numOfEpisodes,))\n",
    "max_reward =  np.zeros((numOfEpisodes,))\n",
    "replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "steps_to_update_target_model = 0\n",
    "\n",
    "\n",
    "for episode in range(numOfEpisodes):\n",
    "    \n",
    "        \n",
    "    print('Episode:', episode)\n",
    "\n",
    "    collisionCnt = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    #state = np.array(state)\n",
    "    \n",
    "                       \n",
    "    for iteration in range(numTimeSlots):\n",
    "        \n",
    "                        \n",
    "        steps_to_update_target_model += 1\n",
    "        \n",
    "        #print(\"Time Slot   :\", iteration)\n",
    "        #print(\"----------------------\")\n",
    "        \n",
    "        #print(\"Estado atual:\", state[0])\n",
    "                  \n",
    "        # Escolha da ação.\n",
    "        if np.random.uniform() >= epsilon:\n",
    "            #predicted = model.predict(state.reshape(1,4))\n",
    "            #action = np.argmax(predicted[0])\n",
    "            action0 = np.argmax(model.predict(state[:,0]-state[:,1])[0])\n",
    "            action1 = np.argmax(model.predict(state[:,1]-state[:,2])[0])\n",
    "            action2 = np.argmax(model.predict(state[:,2]-state[:,3])[0])\n",
    "            action3 = np.argmax(model.predict(state[:,3]-state[:,4])[0])\n",
    "            \n",
    "            #print(\"Action (Q)\") # action0, action1, action2, action3\n",
    "        else:\n",
    "            action0 = np.random.randint(0, cwSize)\n",
    "            action1 = np.random.randint(0, cwSize)\n",
    "            action2 = np.random.randint(0, cwSize)\n",
    "            action3 = np.random.randint(0, cwSize)\n",
    "            #print(\"Action (rnd)\") # action0, action1, action2, action3\n",
    "            \n",
    "        \n",
    "        actionVec = [action0, action1, action2, action3, 100]\n",
    "        #print(\"ActionVec   :\", actionVec)\n",
    "        \n",
    "        next_state, reward, done, _, _ = env.step(actionVec)\n",
    "      \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        #next_state = np.array(next_state)\n",
    "        #print(\"Next State  :\", next_state[0])\n",
    "        \n",
    "        \n",
    "        # Add information to replay buffer.\n",
    "        replay_memory.append([state[0], actionVec, reward, next_state[0], done])          \n",
    "\n",
    "        # Accumulate reward.\n",
    "        episode_reward[episode] += reward\n",
    "      \n",
    "        # Plot received reward.\n",
    "        #print(\"Reward      :\", reward)\n",
    "        #print(\"-------------\", \"\\n\")      \n",
    "\n",
    "        \n",
    "        \n",
    "        # Train\n",
    "        if len(replay_memory) >= MIN_REPLAY_SIZE:\n",
    "            \n",
    "            history = train(replay_memory, model, model_target, done, batch_size, acc_history, loss_history)\n",
    "\n",
    "        # Is it the end of the episode?\n",
    "        if done:\n",
    "            # Update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            print('########## Game Over ##########')\n",
    "            break\n",
    "        \n",
    "        # updates the current state\n",
    "        state = next_state\n",
    "        \n",
    "    # Decrease epsilon along the way.\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "    epsilon_vec[episode] = epsilon \n",
    "        \n",
    "    # Store maxmim possible reward per episode.\n",
    "    max_reward[episode] = iteration + 1              \n",
    "        \n",
    "    #print('episode_reward:', episode_reward[episode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "state = label_binarize([0, 1, 2, 3], classes=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward, label='Actual reward')\n",
    "plt.plot(max_reward, '.', label='Max. reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Accumulated Reward')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d69065",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff  = (max_reward - episode_reward)\n",
    "plt.plot(diff, label='Diff')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Difference')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilon_vec)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(acc_history)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend('train', loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(loss_history)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend('train', loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae7f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
